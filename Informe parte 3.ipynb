{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación del dataset\n",
    "\n",
    "El dataset CIFAR10 es un dataset para experimentar con reconocimiento de imágenes. Consiste en un dataset de 60.000 imágenes pequeñas (32 x 32 pixeles) con 10 clases de objetos (6000 por clase), es decir, cada clase tiene la misma probabilidad.\n",
    "\n",
    "El dataset contiene 50000 ejemplos de entrenamiento y 10000 de prueba. Del conjunto de entrenamiento, se han extraído al azar 10000 ejemplos desde el conjunto de entrenamiento para el conjunto de validación. Por lo que al final se cuenta con 40000 ejemplos de entrenamiento, 10000 de validación y 10000 de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3072)\n",
      "(40000,)\n",
      "(10000, 3072)\n",
      "(10000,)\n",
      "(10000, 3072)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "from scipy.misc import imread\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import os\n",
    " \n",
    "def load_CIFAR_one(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        datadict = pickle.load(f)\n",
    "        X = datadict['data']\n",
    "        Y = datadict['labels']\n",
    "    return X, np.array(Y)\n",
    " \n",
    "def load_CIFAR10(PATH):\n",
    "    xs, ys = [], []\n",
    "    for b in range(1,6):\n",
    "        f = os.path.join(PATH, 'data_batch_%d' % b)\n",
    "        X, Y = load_CIFAR_one(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr = np.concatenate(xs)\n",
    "    Ytr = np.concatenate(ys)\n",
    "    del X, Y\n",
    "    Xte, Yte = load_CIFAR_one(os.path.join(PATH, 'test_batch'))\n",
    " \n",
    "    numero_imagenes = 10000\n",
    "    np.random.seed(2)\n",
    "    seleccion = np.random.choice(50000, numero_imagenes, replace = False)\n",
    " \n",
    "    Xva = np.array(list(Xtr[i] for i in seleccion))\n",
    "    Yva = np.array(list(Ytr[i] for i in seleccion))\n",
    " \n",
    "    Xtr = np.delete(Xtr, seleccion, 0)\n",
    "    Ytr = np.delete(Ytr, seleccion)\n",
    "   \n",
    "    return Xtr, Ytr, Xte, Yte, Xva, Yva\n",
    " \n",
    "#you need to add Xval\n",
    "Xtr, Ytr, Xte, Yte, Xva, Yva = load_CIFAR10('./dataset')\n",
    "\n",
    "print Xtr.shape\n",
    "print Ytr.shape\n",
    "print Xva.shape\n",
    "print Yva.shape\n",
    "print Xte.shape\n",
    "print Yte.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "b) Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente sólo escalando los datos de acuerdo a la intensidad máxima de pixel *(i.e., dividiendo por 255)* y luego centrando y escalándolos como en actividades anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def escalar(data, completo = True):\n",
    "    filas, columnas = data.shape\n",
    "    data = data.astype(np.float64)\n",
    "    if completo:\n",
    "        for i in range(columnas):\n",
    "            data[:filas, i] = (data[:filas, i]- np.mean(data[:filas, i]))/np.std(data[:filas, i])\n",
    "        return data\n",
    "    return data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El conjunto de training original\n",
      "\n",
      "[[154 126 105 ..., 139 142 144]\n",
      " [255 253 253 ...,  83  83  84]\n",
      " [ 28  37  38 ...,  28  37  46]\n",
      " ..., \n",
      " [145 151 156 ...,  56  53  54]\n",
      " [189 186 185 ..., 169 171 171]\n",
      " [229 236 234 ..., 173 162 161]]\n",
      "\n",
      "\n",
      "El conjunto de training escalando de acuerdo a la intesidad maxima\n",
      "\n",
      "[[ 0.60392157  0.49411765  0.41176471 ...,  0.54509804  0.55686275\n",
      "   0.56470588]\n",
      " [ 1.          0.99215686  0.99215686 ...,  0.3254902   0.3254902\n",
      "   0.32941176]\n",
      " [ 0.10980392  0.14509804  0.14901961 ...,  0.10980392  0.14509804\n",
      "   0.18039216]\n",
      " ..., \n",
      " [ 0.56862745  0.59215686  0.61176471 ...,  0.21960784  0.20784314\n",
      "   0.21176471]\n",
      " [ 0.74117647  0.72941176  0.7254902  ...,  0.6627451   0.67058824\n",
      "   0.67058824]\n",
      " [ 0.89803922  0.9254902   0.91764706 ...,  0.67843137  0.63529412\n",
      "   0.63137255]]\n",
      "\n",
      "\n",
      "El conjunto de training escalando de forma normal\n",
      "\n",
      "[[ 0.31999272 -0.05418421 -0.35724693 ...,  0.38885303  0.43148518\n",
      "   0.44895838]\n",
      " [ 1.69385262  1.69638376  1.68843315 ..., -0.47413728 -0.47321003\n",
      "  -0.45852247]\n",
      " [-1.39393152 -1.2809602  -1.28333183 ..., -1.32171705 -1.17856562\n",
      "  -1.03326035]\n",
      " ..., \n",
      " [ 0.19756956  0.29041578  0.34768337 ..., -0.89022189 -0.93322455\n",
      "  -0.9122629 ]\n",
      " [ 0.79608278  0.77285577  0.74852609 ...,  0.85116927  0.87616588\n",
      "   0.85732477]\n",
      " [ 1.34018572  1.46205576  1.42581206 ...,  0.91281144  0.73816153\n",
      "   0.70607796]]\n"
     ]
    }
   ],
   "source": [
    "print \"El conjunto de training original\\n\"\n",
    "print Xtr\n",
    "print\"\\n\"\n",
    "print\"El conjunto de training escalando de acuerdo a la intesidad maxima\\n\"\n",
    "print escalar(Xtr, False)\n",
    "print\"\\n\"\n",
    "print\"El conjunto de training escalando de forma normal\\n\"\n",
    "print escalar(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales artificiales\n",
    "\n",
    ">c) Diseñe, entrene y evalúe una red neuronal para el problema CIFAR a partir de la representación original\n",
    "de las imágenes (píxeles RGB). Experimente con distintas arquitecturas y métodos de entrenamiento, midiendo el error de clasificación sobre el conjunto de validación. En base a esta última medida de desempeñoo, decida qué modelo, de entre todos los evaluados, evaluará finalmente en el conjunto de test.\n",
    "Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o igual a $0.5$.\n",
    "\n",
    "Se entrenó una red neuronal con tres parámetros distintos de *epoch* con $e = \\{ 2, 4, 8 \\}$. Se vió que para el valor $e = 8$ fue el mejor valor, mejorando a medida que se aumenta el valor de *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaler_function(Xtr, Xt, Xva, scale = True):\n",
    "    Xtr, Xt, Xva = Xtr.astype(np.float64), Xt.astype(np.float64), Xva.astype(np.float64)\n",
    "    scaler = StandardScaler(with_std = scale).fit(Xtr)\n",
    "    Xtr_scaled = scaler.transform(Xtr)\n",
    "    Xt_scaled = scaler.transform(Xt)\n",
    "    Xva_scaled = scaler.transform(Xva)\n",
    "    return Xtr_scaled, Xt_scaled, Xva_scaled\n",
    "\n",
    "Xtr_scaled, Xte_scaled, Xva_scaled = scaler_function(Xtr, Xte, Xva)\n",
    "Ytr_categorical = to_categorical(Ytr)\n",
    "Yte_categorical = to_categorical(Yte)\n",
    "Yva_categorical = to_categorical(Yva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 529s - loss: 0.2919 - acc: 0.9008 - val_loss: 0.2727 - val_acc: 0.9022\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 518s - loss: 0.2680 - acc: 0.9031 - val_loss: 0.2607 - val_acc: 0.9041\n",
      "40000/40000 [==============================] - 244s   \n",
      "10000/10000 [==============================] - 61s    \n",
      "\n",
      "Epoch 2 ---> Va. Acc 0.904100   Va. Error 0.095900 | Tr. Acc 0.904265   Tr. Error 0.095735\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "40000/40000 [==============================] - 528s - loss: 0.2918 - acc: 0.9008 - val_loss: 0.2738 - val_acc: 0.9021\n",
      "Epoch 2/4\n",
      "40000/40000 [==============================] - 534s - loss: 0.2678 - acc: 0.9031 - val_loss: 0.2609 - val_acc: 0.9044\n",
      "Epoch 3/4\n",
      "40000/40000 [==============================] - 544s - loss: 0.2576 - acc: 0.9052 - val_loss: 0.2529 - val_acc: 0.9061\n",
      "Epoch 4/4\n",
      "40000/40000 [==============================] - 520s - loss: 0.2507 - acc: 0.9067 - val_loss: 0.2478 - val_acc: 0.9073\n",
      "40000/40000 [==============================] - 248s   \n",
      "10000/10000 [==============================] - 61s    \n",
      "\n",
      "Epoch 4 ---> Va. Acc 0.907300   Va. Error 0.092700 | Tr. Acc 0.908320   Tr. Error 0.091680\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "40000/40000 [==============================] - 536s - loss: 0.2901 - acc: 0.9010 - val_loss: 0.2722 - val_acc: 0.9024\n",
      "Epoch 2/8\n",
      "40000/40000 [==============================] - 542s - loss: 0.2667 - acc: 0.9033 - val_loss: 0.2596 - val_acc: 0.9043\n",
      "Epoch 3/8\n",
      "40000/40000 [==============================] - 543s - loss: 0.2575 - acc: 0.9051 - val_loss: 0.2522 - val_acc: 0.9060\n",
      "Epoch 4/8\n",
      "40000/40000 [==============================] - 525s - loss: 0.2511 - acc: 0.9066 - val_loss: 0.2472 - val_acc: 0.9075\n",
      "Epoch 5/8\n",
      "40000/40000 [==============================] - 516s - loss: 0.2461 - acc: 0.9080 - val_loss: 0.2437 - val_acc: 0.9084\n",
      "Epoch 6/8\n",
      "40000/40000 [==============================] - 539s - loss: 0.2419 - acc: 0.9094 - val_loss: 0.2401 - val_acc: 0.9098\n",
      "Epoch 7/8\n",
      "40000/40000 [==============================] - 545s - loss: 0.2389 - acc: 0.9103 - val_loss: 0.2374 - val_acc: 0.9107\n",
      "Epoch 8/8\n",
      "40000/40000 [==============================] - 542s - loss: 0.2358 - acc: 0.9111 - val_loss: 0.2348 - val_acc: 0.9115\n",
      "40000/40000 [==============================] - 258s   \n",
      "10000/10000 [==============================] - 64s    \n",
      "\n",
      "Epoch 8 ---> Va. Acc 0.911450   Va. Error 0.088550 | Tr. Acc 0.913365   Tr. Error 0.086635\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_validate_epoch = []\n",
    "error_train_epoch = []\n",
    "\n",
    "for k in range(1,4):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, init='uniform', activation='softmax'))\n",
    "    model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(Xtr_scaled, Ytr_categorical, nb_epoch=2**k, batch_size=100, verbose=1, validation_data=(Xva_scaled, Yva_categorical))\n",
    "\n",
    "    scores = model.evaluate(Xtr_scaled, Ytr_categorical)\n",
    "    tr_acc = scores[1]\n",
    "    \n",
    "    scores = model.evaluate(Xva_scaled, Yva_categorical)\n",
    "    va_acc = scores[1]\n",
    "    \n",
    "    print \"\\nEpoch %d ---> Va. Acc %f   Va. Error %f | Tr. Acc %f   Tr. Error %f\" %(2**k, va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "    print\"-----------------------------------------------------------------------------------------\\n\"\n",
    "    error_validate_epoch.append(1 - va_acc)\n",
    "    error_train_epoch.append(1 - tr_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al observar el gráfico se puede ver que el error pareciera disminuir linealmente. Se deberían hacer más pruebas con más posibles valores para el parámetro para verificar esta afirmación pues los errores no varían tanto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvwAAAGgCAYAAADW/LOeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nU+EXEREREQlhKvwiIiIiIiHs/wEVhAUO6OhTfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xeff10b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "potencias = [2**x for x in range(1, 4)]\n",
    "\n",
    "plt.plot(potencias, error_train_epoch, 'go--', label='Train Error')\n",
    "plt.plot(potencias, error_validate_epoch, 'go-', label= 'Validation Error')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.title('Error vs Epoch', y=1.08)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se probó variando el número $n$ de neuronas con $n = \\{ 100, 150, 200\\}$. Nuevamente los errores no varían tanto, pero el que dió mejor fue usando 200 neuronas lo cual es esperable puesto que con más neuronas el modelo puede aprender más de los datos. Sin embargo, la mejora no es muy significativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 537s - loss: 0.2914 - acc: 0.9008 - val_loss: 0.2731 - val_acc: 0.9022\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 625s - loss: 0.2675 - acc: 0.9033 - val_loss: 0.2604 - val_acc: 0.9046\n",
      "40000/40000 [==============================] - 878s - loss: 0.2873 - acc: 0.9011 - val_loss: 0.2692 - val_acc: 0.9031\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 868s - loss: 0.2639 - acc: 0.9039 - val_loss: 0.2570 - val_acc: 0.9052\n",
      "40000/40000 [==============================] - 386s   \n",
      "10000/10000 [==============================] - 95s    \n",
      "\n",
      "Neuronas 150 ---> Va. Acc 0.905220   Va. Error 0.094780 | Tr. Acc 0.905755   Tr. Error 0.094245\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 1055s - loss: 0.2852 - acc: 0.9013 - val_loss: 0.2670 - val_acc: 0.9029\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 1087s - loss: 0.2613 - acc: 0.9046 - val_loss: 0.2546 - val_acc: 0.9061\n",
      "40000/40000 [==============================] - 491s   \n",
      "10000/10000 [==============================] - 125s   \n",
      "\n",
      "Neuronas 200 ---> Va. Acc 0.906060   Va. Error 0.093940 | Tr. Acc 0.906520   Tr. Error 0.093480\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "error_validate_epoch = []\n",
    "error_train_epoch = []\n",
    "\n",
    "for k in [100, 150, 200]:\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(k, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, init='uniform', activation='softmax'))\n",
    "    model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(Xtr_scaled, Ytr_categorical, nb_epoch=2, batch_size=100, verbose=1, validation_data=(Xva_scaled, Yva_categorical))\n",
    "\n",
    "    scores = model.evaluate(Xtr_scaled, Ytr_categorical)\n",
    "    tr_acc = scores[1]\n",
    "    \n",
    "    scores = model.evaluate(Xva_scaled, Yva_categorical)\n",
    "    va_acc = scores[1]\n",
    "    \n",
    "    print \"\\nNeuronas %d ---> Va. Acc %f   Va. Error %f | Tr. Acc %f   Tr. Error %f\" %(k, va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "    print\"-----------------------------------------------------------------------------------------\\n\"\n",
    "    \n",
    "    error_validate_epoch.append(1 - va_acc)\n",
    "    error_train_epoch.append(1 - tr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot([5, 10, 15], error_train_epoch, 'go--', label='Train Error')\n",
    "plt.plot([5, 10, 15], error_validate_epoch, 'go-', label= 'Validation Error')\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.title('Error vs Epoch', y=1.08)\n",
    "plt.xlabel('Neuronas Ocultas')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se probó cambiando la función de activación probando con *softmax*, *sigmoid* y *relu*. El mejor fue *softmax*, seguido de cerca por *sigmoid* ambos con un *accuracy* cercano a 0.90 en el conjunto de validación. La función de activación *relu*, sin embargo, tuvo un desempeño más decadente logrando solamente un *accuracy* de 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 520s - loss: 0.2922 - acc: 0.9006 - val_loss: 0.2734 - val_acc: 0.9021\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 521s - loss: 0.2674 - acc: 0.9032 - val_loss: 0.2602 - val_acc: 0.9046\n",
      "40000/40000 [==============================] - 242s   \n",
      "10000/10000 [==============================] - 60s    \n",
      "\n",
      "Softmax ---> Va. Acc 0.904560   Va. Error 0.095440 | Tr. Acc 0.904722   Tr. Error 0.095278\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 523s - loss: 0.3180 - acc: 0.8962 - val_loss: 0.2863 - val_acc: 0.9021\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 521s - loss: 0.2800 - acc: 0.9020 - val_loss: 0.2732 - val_acc: 0.9032\n",
      "40000/40000 [==============================] - 244s   \n",
      "10000/10000 [==============================] - 61s    \n",
      "\n",
      "Sigmoid ---> Va. Acc 0.903180   Va. Error 0.096820 | Tr. Acc 0.902910   Tr. Error 0.097090\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "40000/40000 [==============================] - 559s - loss: 2.7817 - acc: 0.8114 - val_loss: 2.8870 - val_acc: 0.8099\n",
      "Epoch 2/2\n",
      "40000/40000 [==============================] - 557s - loss: 2.8386 - acc: 0.8100 - val_loss: 2.8874 - val_acc: 0.8099\n",
      "40000/40000 [==============================] - 252s   \n",
      "10000/10000 [==============================] - 62s    \n",
      "\n",
      "Relu ---> Va. Acc 0.809940   Va. Error 0.190060 | Tr. Acc 0.810017   Tr. Error 0.189983\n",
      "-----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtr_scaled, Ytr_categorical, nb_epoch= 2, batch_size=100, verbose=1, validation_data=(Xva_scaled, Yva_categorical))\n",
    "\n",
    "scores = model.evaluate(Xtr_scaled, Ytr_categorical)\n",
    "tr_acc = scores[1]\n",
    "\n",
    "scores = model.evaluate(Xva_scaled, Yva_categorical)\n",
    "va_acc = scores[1]\n",
    "\n",
    "\n",
    "softmax = (va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print \"\\nSoftmax ---> Va. Acc %f   Va. Error %f | Tr. Acc %f   Tr. Error %f\" %(va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print\"-----------------------------------------------------------------------------------------\\n\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, init='uniform', activation='sigmoid'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtr_scaled, Ytr_categorical, nb_epoch= 2, batch_size=100, verbose=1, validation_data=(Xva_scaled, Yva_categorical))\n",
    "\n",
    "scores = model.evaluate(Xtr_scaled, Ytr_categorical)\n",
    "tr_acc = scores[1]\n",
    "\n",
    "scores = model.evaluate(Xva_scaled, Yva_categorical)\n",
    "va_acc = scores[1]\n",
    "\n",
    "sigmoid = (va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print \"\\nSigmoid ---> Va. Acc %f   Va. Error %f | Tr. Acc %f   Tr. Error %f\" %(va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print\"-----------------------------------------------------------------------------------------\\n\"\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=Xtr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(10, init='uniform', activation='relu'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(Xtr_scaled, Ytr_categorical, nb_epoch= 2, batch_size=100, verbose=1, validation_data=(Xva_scaled, Yva_categorical))\n",
    "\n",
    "scores = model.evaluate(Xtr_scaled, Ytr_categorical)\n",
    "tr_acc = scores[1]\n",
    "\n",
    "scores = model.evaluate(Xva_scaled, Yva_categorical)\n",
    "va_acc = scores[1]\n",
    "\n",
    "relu = (va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print \"\\nRelu ---> Va. Acc %f   Va. Error %f | Tr. Acc %f   Tr. Error %f\" %(va_acc, 1 - va_acc, tr_acc, 1 - tr_acc)\n",
    "print\"-----------------------------------------------------------------------------------------\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM No Lineal\n",
    "\n",
    ">(e) Entrene una SVM no lineal sobre los pixeles originales y sobre los atributos de alto nivel utilizados para\n",
    "representar las imágenes en el item anterior. Puede utilizar el conjunto de validación para seleccionar\n",
    "hiper-parámetros, como el nivel de regularización aplicado y/o la función de kernel a utilizar.\n",
    "\n",
    "Para el entrenamiento de la SVM no lineal se consideraron los pixeles originales de las imágenes junto a las características de alto nivel (histogramas de tono y descriptores HOG) con escalamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:14:21.214893\n",
      "(40000, 32, 32, 3)\n",
      "22:15:43.075564\n",
      "(10000, 32, 32, 3)\n",
      "22:16:03.425401\n",
      "(10000, 32, 32, 3)\n",
      "22:16:24.316927\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from top_level_features import hog_features\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import extract_features\n",
    "import datetime\n",
    "\n",
    "Xtr, Ytr, Xte, Yte, Xva, Yva = load_CIFAR10('./dataset')\n",
    "\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xtr,[hog_features, color_histogram_hsv])\n",
    "Xtrf = np.concatenate((Xtr,feat),axis=1)\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xva,[hog_features, color_histogram_hsv])\n",
    "Xvaf = np.concatenate((Xva,feat),axis=1)\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xte,[hog_features, color_histogram_hsv])\n",
    "Xtef = np.concatenate((Xte,feat),axis=1)\n",
    "print datetime.datetime.now().time()\n",
    "\n",
    "scaler = StandardScaler().fit(Xtrf)\n",
    "Xtrs = scaler.transform(Xtrf)\n",
    "Xvas = scaler.transform(Xvaf)\n",
    "Xtes = scaler.transform(Xtef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se selecciono el kernel a utilizar y el parámetro de regularización C con un método iterativo. Primero, se probó cual de los dos kernels a utilizar con $C=1.0$. Luego de seleccionado el kernel, se probó cual valor para $C$ era mejor con $C \\in \\{ 0.1, 1, 10 \\}$. Cabe notar que por limitaciones de tiempo se limitó la ejecución del solver interno de la SVM a 300 iteraciones.\n",
    "\n",
    "Se obtuvó así una SVM no lineal haciendo uso de un kernel Gaussiano con un C = 1.0. El *accuracy* en el conjunto de entrenamiento fue de 0.4732, en el conjunto de validación de 0.4261 y en el conjunto de pruebas 0.4253. Este resultado podría mejorarse haciendo uso de más iteraciones en el solver o dejar que la optimización se complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:22:03.658636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/worm/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.py:224: ConvergenceWarning: Solver terminated early (max_iter=300).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel=poly, C=1.000000, score=0.143400\n",
      "22:48:32.655011\n",
      "Kernel=rbf, C=1.000000, score=0.426100\n",
      "23:18:50.436049\n",
      "Kernel=rbf, C=0.100000, score=0.328100\n",
      "23:47:43.741294\n",
      "Kernel=rbf, C=10.000000, score=0.398400\n",
      "00:18:50.089056\n",
      "7006.668371 segundos\n",
      "Mejor kernel: rbf\n",
      "Mejor C: 1.0\n",
      "Score train: 0.4732\n",
      "Score valid: 0.4261\n",
      "Score test: 0.4253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_best_SVC(Xtr,Ytr,Xva,Yva):\n",
    "    kernels = ['poly','rbf']\n",
    "    possible_C = [0.1,10.0] #C=1.0 se prueba cuando se prueban los kernels\n",
    "    best_score = 0\n",
    "    best_model = SVC()\n",
    "    best_kernel = 'rbf'\n",
    "    best_C = 1.0\n",
    "    print datetime.datetime.now().time()\n",
    "    #primero seleccionamos el kernel\n",
    "    for k in kernels:\n",
    "        model = SVC(kernel = k, random_state = 0, max_iter = 300)\n",
    "        model.fit(Xtr, Ytr)\n",
    "        score = model.score(Xva,Yva)\n",
    "        if( score >= best_score ):\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_kernel = k\n",
    "            best_C = 1.0\n",
    "        print \"Kernel=%s, C=%f, score=%f\"%(k,1.0,score)\n",
    "        print datetime.datetime.now().time()\n",
    "    \n",
    "    #luego seleccionamos C\n",
    "    for Ct in possible_C:\n",
    "        model = SVC(kernel = best_kernel, C=Ct, random_state = 0, max_iter = 300)\n",
    "        model.fit(Xtr, Ytr)\n",
    "        score = model.score(Xva,Yva)\n",
    "        if( score >= best_score ):\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_C = Ct\n",
    "        print \"Kernel=%s, C=%f, score=%f\"%(best_kernel,Ct,score)\n",
    "        print datetime.datetime.now().time()\n",
    "    return best_model, best_kernel, best_C, best_score\n",
    "    \n",
    "start = time.time()\n",
    "best_SVC, best_kernel, best_C, best_SVC_score = get_best_SVC(Xtrs,Ytr,Xvas,Yva)\n",
    "end = time.time()\n",
    "print \"%f segundos\"%(end-start)\n",
    "print \"Mejor kernel:\", best_kernel\n",
    "print \"Mejor C:\", best_C\n",
    "print \"Score train:\",best_SVC.score(Xtrs, Ytr)\n",
    "print \"Score valid:\",best_SVC_score\n",
    "print \"Score test:\",best_SVC.score(Xtes, Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al entrenar la SVM completa con los parámetros obtenidos previamente se puede mejorar bastante el resultado. El *accuracy* obtenido fue de 0.7961 para el conjunto de entrenamiento, 0.6237 para el conjunto de validación y de 0.618 para el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:29:02.806582\n",
      "10706.278124 segundos\n",
      "Mejor kernel: rbf\n",
      "Mejor C: 1.0\n",
      "Score train: 0.79615\n",
      "Score valid: 0.6237\n",
      "Score test: 0.618\n",
      "06:12:11.883237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "import time\n",
    "import datetime\n",
    "   \n",
    "print datetime.datetime.now().time()\n",
    "start = time.time()\n",
    "model_SVC = SVC(kernel = best_kernel, C=best_C, random_state = 0)\n",
    "model_SVC.fit(Xtrs,Ytr)\n",
    "end = time.time()\n",
    "print \"%f segundos\"%(end-start)\n",
    "print \"Mejor kernel:\", best_kernel\n",
    "print \"Mejor C:\", best_C\n",
    "print \"Score train:\",best_SVC.score(Xtrs, Ytr)\n",
    "print \"Score valid:\",best_SVC.score(Xvas, Yva)\n",
    "print \"Score test:\",best_SVC.score(Xtes, Yte)\n",
    "print datetime.datetime.now().time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árbol de Decisión\n",
    "\n",
    ">(f) Entrene un árbol de clasificación sobre los pixeles originales y sobre los atributos de alto nivel utilizados para representar las imágenes en el item anterior. Puede utilizar el conjunto de validación para seleccionar hiper-parámetros, como la profundidad máxima del árbol.\n",
    "\n",
    "Para el entrenamiento del árbol se consideraron los pixeles originales de las imágenes junto a las características de alto nivel (histogramas de tono y descriptores HOG) sin escalamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:49:31.505173\n",
      "(40000, 32, 32, 3)\n",
      "15:50:53.801199\n",
      "(10000, 32, 32, 3)\n",
      "15:51:14.402433\n",
      "(10000, 32, 32, 3)\n",
      "15:51:35.538437\n"
     ]
    }
   ],
   "source": [
    "from top_level_features import hog_features\n",
    "from top_level_features import color_histogram_hsv\n",
    "from top_level_features import extract_features\n",
    "import datetime\n",
    "\n",
    "Xtr, Ytr, Xte, Yte, Xva, Yva = load_CIFAR10('./dataset')\n",
    "\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xtr,[hog_features, color_histogram_hsv])\n",
    "Xtrf = np.concatenate((Xtr,feat),axis=1)\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xva,[hog_features, color_histogram_hsv])\n",
    "Xvaf = np.concatenate((Xva,feat),axis=1)\n",
    "print datetime.datetime.now().time()\n",
    "feat = extract_features(Xte,[hog_features, color_histogram_hsv])\n",
    "Xtef = np.concatenate((Xte,feat),axis=1)\n",
    "print datetime.datetime.now().time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se limitó la altura del árbol. Previamente se probó con parámetros por defecto y se obtuvó un árbol de 44 niveles de profundidad el cual generaba un serio sobre-ajuste (*accuracy* de 1.0 en el conjunto de entrenamiento y de 0.2 en el conjunto de pruebas).\n",
    "\n",
    "Para seleccionar una buena profundidad se seleccionó la profundidad máxima $d \\in \\{2,4,8,16,32\\}$ que tuviera mejor *accuracy* sobre el conjunto de validación. \n",
    "\n",
    "El mejor árbol encontrado fue con 8 niveles de profundidad. Si la profundidad del árbol hubiese sido más grande no empeoraba considerablemente la *accuracy* sobre el conjunto de validación. Con 16 y 32 niveles la *accuracy* era apenas un 0.1 y 0.2 menor que con 8 niveles, respectivamente.\n",
    "\n",
    "El árbol de 8 niveles obtenido logró un *accuracy* cercano a 0.3 en todos los conjuntos, con un *accuracy* de 0.3757 en el conjunto de entrenamiento, de 0.3075 en el conjunto de validación y de 0.3177 en el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:52:48.145623\n",
      "Depth=2, score=0.190100\n",
      "14:53:11.751494\n",
      "Depth=4, score=0.243700\n",
      "14:53:57.763704\n",
      "Depth=8, score=0.307500\n",
      "14:55:36.248859\n",
      "Depth=16, score=0.292500\n",
      "14:59:02.498599\n",
      "Depth=32, score=0.281000\n",
      "15:02:55.944288\n",
      "607.799025 segundos\n",
      "Mejor profundidad: 8\n",
      "Score train: 0.3757\n",
      "Score valid: 0.3075\n",
      "Score test: 0.3177\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as CTree\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def get_best_tree(Xtr,Ytr,Xva,Yva):\n",
    "    #se probó previamente que la profundidad por defecto quedaba en 44\n",
    "    #con un serio over-fitting siendo peor su rendimiento que un arbol testeado con profundidad 10\n",
    "    depths = [2,4,8,16,32]\n",
    "    best_score = 0\n",
    "    best_model = CTree()\n",
    "    best_depth = 0\n",
    "    print datetime.datetime.now().time()\n",
    "    for depth in depths:\n",
    "        model = CTree(max_depth = depth)\n",
    "        model.fit(Xtr, Ytr)\n",
    "        score = model.score(Xva,Yva)\n",
    "        if( score >= best_score ):\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_depth = depth\n",
    "        print \"Depth=%d, score=%f\"%(depth,score)\n",
    "        print datetime.datetime.now().time()\n",
    "    return best_model, best_depth, best_score\n",
    "\n",
    "start = time.time()\n",
    "best_model, best_depth, best_score = get_best_tree(Xtrf,Ytr,Xvaf,Yva)\n",
    "end = time.time()\n",
    "print \"%f segundos\"%(end-start)\n",
    "print \"Mejor profundidad:\", best_depth\n",
    "print \"Score train:\",best_model.score(Xtrf, Ytr)\n",
    "print \"Score valid:\",best_score\n",
    "print \"Score test:\",best_model.score(Xtef, Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusiones\n",
    "\n",
    "Se probaron varios clasificadores no lineales para el dataset CIFAR10 que contempla clasificación de imagenes. Los clasificadores probados fueron: ANN (red neuronal artificial), SVM y Árbol de clasificación\n",
    "\n",
    "El mejor clasificador pareciera ser las ANN con un *accuracy* cercano al 90% sobre el conjunto de validación.\n",
    "\n",
    "Luego, la SVM no lineal logró un *accuracy* de 0.623 sobre el conjunto de validación y de 0.618 sobre el conjunto de pruebas. Se debe notar que el ajuste de parámetros se realizó limitando el número de iteraciones para la optimización.\n",
    "\n",
    "Finalmente, el árbol de decisión tuvo el peor desempeño con un *accuracy* de 0.3075 en el conjunto de validación y de 0.3177 sobre el conjunto de pruebas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
